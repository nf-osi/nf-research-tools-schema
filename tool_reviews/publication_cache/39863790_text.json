{
  "pmid": "PMID:39863790",
  "title": "A multicenter study of neurofibromatosis type 1 utilizing deep learning for whole body tumor identification.",
  "abstract": "Deep-learning models have shown promise in differentiating between benign and malignant lesions. Previous studies have primarily focused on specific anatomical regions, overlooking tumors occurring throughout the body with highly heterogeneous whole-body backgrounds. Using neurofibromatosis type 1 (NF1) as an example, this study developed highly accurate MRI-based deep-learning models for the early automated screening of malignant peripheral nerve sheath tumors (MPNSTs) against complex whole-body background. In a Chinese seven-center cohort, data from 347 subjects were analyzed. Our one-step model incorporated normal tissue/organ labels to provide contextual information, offering a solution for tumors with complex backgrounds. To address privacy concerns, we utilized a lightweight deep neural network suitable for hospital deployment. The final model achieved an accuracy of 85.71% for MPNST diagnosis in the validation cohort and 84.75% accuracy in the independent test set, outperforming another classic two-step model. This success suggests potential for AI models in screening other whole-body primary/metastatic tumors.",
  "authors": "Cheng-Jiang Wei; Yan Tang; Yang-Bai Sun; Tie-Long Yang; Cheng Yan; Hui Liu; Jun Liu; Jing-Ning Huang; Ming-Han Wang; Zhen-Wei Yao; Ji-Long Yang; Zhi-Chao Wang; Qing-Feng Li",
  "journal": "NPJ digital medicine",
  "publicationDate": "2025-01-26",
  "doi": "10.1038/s41746-025-01454-z",
  "methods": "Methods Study design and participants This multicenter study used MRI image sets from seven hospitals in China, including Shanghai Ninth People\u2019s Hospital, Shanghai Jiaotong University School of Medicine; Fudan University Shanghai Cancer Center; Shanghai Sixth People\u2019s Hospital, Shanghai Jiaotong University School of Medicine,; Zhongshan Hospital, Fudan University; Huashan Hospital, Fudan University; Tianjin Medical University Cancer Institute and Hospital, Tianjin Medical University; The Fourth Hospital of Hebei Medical University/Hebei Tumor Hospital. Cross-sectional MRI image sets with T2-weighted sequences of patients from 2010 to 2021 with pathologically confirmed diagnoses of PNF/MPNST were eligible for inclusion. Poor quality and nondiagnostic MRI image sets were excluded. There were no limitations on the number of MRI image sets from one individual that could be included, and some image sets included more than one tumor lesion. Given that PNF/MPNST can occur in various body regions among different patients, MRI image sets of different body regions were included as long as the existence of tumor lesions was confirmed. Additionally, another independent MRI image set was adopted for model testing. This multicenter study adhered to the principles of the Declaration of Helsinki and received approval from the institutional review board of Shanghai Ninth People\u2019s Hospital, Shanghai Jiao Tong University School of Medicine (SH9H-2019-T163-2, SH9H-2019-T163-5). As patients were retrospectively recruited, informed consent was exempted by the review board. Clinical manifestations The clinical manifestations of the individuals from Shanghai Ninth People\u2019s Hospital were abstracted for analysis and included age at each MRI examination, sex, pathological diagnosis (PNF or MPNST), presence of spontaneous persistent pain, motor deficits, hard or soft texture, and whether rapid growth was reported in a short period (reported by the patients or their parents). This inclusion was also approved by the aforementioned review board. As the collection of clinical manifestations in other centers was not included in the above ethical approval, we did not include this information in our study for ethical concerns. Furthermore, we have ensured that any data shared in the study cannot be traced back to individual patients, thereby upholding their confidentiality. Image acquisition MRI scans were accumulated within the Picture Archiving and Communication System (PACS) across all participating centers for each patient. In the context of a retrospective multi-center study, inter-site variability emerged due to disparate devices and acquisition protocols, presenting a significant challenge in these types of studies. In this study, the MRI scanners included devices from GE Medical Systems, Siemens, Philips Medical Systems, and United Imaging Healthcare; the magnetic field strengths included 1.5\u2009T and 3.0\u2009T; slice thicknesses varied among different body regions, including 3\u2009mm, 4\u2009mm, 5\u2009mm, 5.5\u2009mm, 6\u2009mm, etc., with or without diffusion-weighted imaging sequences and contrast-enhanced T1-weighted imaging sequences. Given the large diversity in MRI scanners and the fact that this study only used axial T2-weighted fat suppression sequences, we only described the detailed parameters of the axial T2-weighted fat suppression sequence in the four most frequently used MRI devices in this study (Supplementary Table  4 ). Since the parameter varied not only across different devices but also across different body regions, this table used head and neck MRI as an example. Even though the efficacy may have marginally been affected, we believe that the use of a diverse dataset also has its advantages, such as increased generalizability and applicability to real-world scenarios where multiple imaging devices are often used. Image slice selection and quality control Axial slices of MRI with typical tumor lesions were selected. All MRI scans and clinical data were carefully reviewed by a board-certified radiologist and a doctor within the plastic and reconstructive surgery department, both with expertise in NF1. Manual image segmentation and annotation Ground truth for the ROI region was manually established in all selected images. This task was completed by two experienced radiologists (Y.T. and C.Y.) and one doctor (C.W.) with expertise in NF1, using RectLabel (Ryo Kawamura, San Francisco, USA) and Colabeler (Kuaiyi Technology Co., Ltd, Hangzhou, China). All three readers were required to delineate and label the tumor lesions and other tissues/organs that they deemed essential. The number of these labels was then counted and sorted in descending order. The top 10 labels were retained, and all 10 labels were cross approved by all three readers. The classic two-step deep-learning model development U-Net, a convolutional neural network (CNN), was utilized to perform automatic segmentation . The runtime environment for our model was on Python 3.7.1, and the model structure is illustrated in Fig.  30 3 . Among all included images, a batch consisted of 4 randomly selected slices. Each time, the model obtained one batch for training, and an epoch referred to all the images being learned at once. The automatically segmented results from the U-Net model were randomly partitioned into the training (1031; 665 MPNST and 366 PNF) and test (35; 17 MPNST and 18 PNF) sets. ResNet 18, an 18-layer deep CNN model, was employed for our differential diagnosis model (the model structure is shown in Fig.  3 ) . The model was developed using PyTorch. 31 The one-step deep learning model development for whole-body tumor identification Another deep learning algorithm, You Only Look Once-v5 (YOLO-v5), was adopted and improved to establish a PNF/MPNST detection and diagnosis system. The model structure is shown in Supplementary Fig.  4 , while the training and prediction process is shown in Supplementary Fig.  5 . In the training process, we developed two distinct models: one with only PNF/MPNST labels and the other with all ten labels included. Due to their similarities on T2-weighted images, the gland and lymph tissue were combined into a single label. Three different loss values were calculated for performance evaluation: box loss, object loss, and class loss. The one-step deep learning model used CIOU (Completed IoU) to calculate the box loss. Model interpretability The Gradient-weighted Class Activation Mapping can visualize the predictive basis of CNN-based deep learning models by generating saliency maps, which was normally adopted as a method to visually explain these models . By the Grad-CAM, it was possible to observe which regions in the MRI contributed to the MPNST diagnosis. 32 Statistical analysis The included clinical symptoms were all categorical variables, and the chi-squared test or Fisher\u2019s exact test was used for different types of data.  P  values\u2009<\u20090.05 were considered statistically significant, and  P  values\u2009<\u20090.01 were considered highly statistically significant.",
  "cache_level": "full",
  "has_fulltext": true,
  "fetch_date": "2026-02-19 09:30:53",
  "introduction": "Introduction Deep learning-based artificial intelligence (AI) has recently been recognized for its superior capability in interpreting imaging data and providing accurate radiology assessments . AI models have been developed to identify both benign and malignant lesions in various cancer types, such as breast cancer 1 , lung cancer 2 , and thyroid tumor 3 . Nonetheless, the majority of these deep-learning models have concentrated on lesions within specific regions, while studies developing AI models capable of identifying tumor lesions against a whole-body background are scarce. The principal challenge lies in the dissimilarity of the whole-body background, which constrains the applicability of these models in identifying metastatic tumors or those that may occur throughout the body. 4 Neurofibromatosis type 1 (NF1) is an autosomal dominant genetic disorder with a worldwide incidence ranging from 1:2500 to 1:3000 individuals . Plexiform neurofibromas (PNFs), which occur all over the body, are a hallmark of NF1 patients and are generally not life-threatening 5 , 6 . However, 8\u201313% of PNFs face the risk of transforming into malignant peripheral nerve sheath tumors (MPNSTs), which have a five-year disease-specific survival rate of approximately 63% 7 . Early detection and surgical resection are crucial for improving prognosis. 8 \u2013 11 Only very recently have radiology imaging methods begun to show promise in the differential diagnosis of MPNST from PNF. Previous studies have indicated that several MRI features could serve as key indicators for distinguishing MPNSTs from benign neurofibromas , but subsequent studies revealed the limited diagnostic accuracy of these morphological features 12 . Other studies demonstrated that the utilization of functional MRI (fMRI) for the differentiation of MPNST and benign PNF yielded improved performance 13 . However, these modalities are not always available in remote areas, and the precision cutoff established in one center may not be applicable to other institutions 13 , 14 . All these factors suggest an urgent need for objective, accurate, and more accessible methods for early screening of MPNST. 13 , 15 Deep-learning-powered AI models offer a potential solution. Regrettably, no such deep-learning approaches have been reported in NF1, potentially due to the limited scale of the dataset as a rare type of tumor. Moreover, unlike common cancers with relatively fixed locations, such as primary lung cancer and breast cancer, NF1 tumors will occur all over the body as a genetic disorder, which confuses the AI model with varied backgrounds. This study aimed to develop T2-weighted MR imaging-based AI algorithms to differentiate between PNF and MPNST lesions. We utilized a multicenter cohort of patients with PNF/MPNST to obtain a sufficient amount of data, making this as the first and largest Chinese population-based study in this domain. Addressing privacy concerns, lightweight deep neural networks were chosen to create an AI model suitable for completely deployment in hospitals with limited computational resources. Furthermore, we managed to incorporate information from surrounding normal organs/tissues into the learning algorithm, thereby improving the identification of tumor lesions all over the body. This advancement could represent a significant milestone in the early clinical screening of MPNST and offer a novel approach for the \u201ccomplete automatic\u201d identification of whole-body background tumors, such as metastatic cancers. A diagram illustrating this workflow is shown in Fig.  1 . Fig. 1 A flowchart of the study process of our MRI-based model development. We developed automatic segmentation and classification deep-learning AI models for the differential diagnosis of MPNST from benign PNF. In the training process, we introduced the surrounding information to augment their performance. The Fig. 1 was created with Biorender.com.",
  "results": "Results Patient characteristics A total of 347 subjects (251 PNF image sets, 96 MPNST image sets) were retrospectively included from 211 individuals (137 PNFs, 74 MPNSTs) who underwent MRI imaging examinations at 7 Chinese centers. All participants had a definite diagnosis of MPNST/PNF confirmed by pathology. Moreover, for individuals who might undergo MRI several times at different ages, we concluded the characteristics according to each \u201cimage set\u201d and referred to a \u201cpatient\u201d as an individual related to one \u201cimage set\u201d in this article for convenience. This approach ensured that the clinical characteristics aligned more accurately with the image set. The detailed characteristics are presented in Table  1 , and the detailed numbers of imaging sets from each center are shown in Supplementary Table  1 . The number of MRI image sets from each body region is also presented in Table  1 . Moreover, for model performance validation, another independent cohort of 20 subjects from individuals (10 PNFs, 10 MPNSTs) was included for the final model test (Fig.  2 ). Fig. 2 A flowchart shows the study sample. The study included 347 MRI image sets from seven Chinese centers, and the details of each training/testing dataset were presented in the flowchart. Table 1 Patients\u2019 characteristics and dataset information Characteristics MPNST PNF Patients (Dataset) 96 251 Age a  (years) 48(29\u201362) 16(12\u201330) Male 46 130 Head and Neck b 27 215 Upper Extremity 14 3 Torso 29 16 Lower Extremity 22 11 Feet 6 1 MPNST  Malignant peripheral nerve sheath tumor,  PNF  Plexiform neurofibroma. a Data are medians, with interquartile ranges in parentheses. b Some patients might have tumors in multiple body regions. T2-weighted images displaying typical tumor lesions were selected, resulting in 3150 images (2141 PNFs, 1009 MPNSTs) being chosen as dataset 1. Then, 1066 uniform size (512\u2009*\u2009512 pixels) images (384 PNFs, 682 MPNSTs) were copied and incorporated in dataset 2. Finally, 123 images (60 PNFs, 63 MPNSTs) from the independent test set were included as dataset 3. The classic two-step deep-learning model based on U-Net and ResNet18 successfully distinguished MPNST from PNF Dataset 2 served as the training set for the U-Net model. Only PNF/MPNST labels remained in these images. After 33,000 rounds of training, accurate segmentation of PNF/MPNST lesions was achieved using our U-Net-based automated approach. Figure  3b  illustrates the comparison of automated segmentation at 1000, 15,000, and 33,000 iterations with the manual process. After 33,000 training iterations, the model demonstrated satisfactory segmentation performance, which was approved by two radiologists for its potential future clinical use. Fig. 3 The classic two-step deep-learning model based on U-Net and ResNet. a  The U-Net model structure, alongside the curves for IoU and Dice Similarity Coefficient, plotted against the epochs in the U-Net model. During the training process, the IoU increased from 0.22 (epoch = 0) to 0.79 (epoch = 100), while the Dice scores exhibited a similar trend.  b  The training process of the U-Net model (Times\u2009=\u20091000/15,000/33,000). The final prediction abilities of the U-Net model were in high accordance with the manual segmentation.  c  The ResNet model structure and the results of prediction in the test set. The prediction results of the ResNet18 model on the test set had an accuracy of 0.743, a recall of 0.882, and a specificity of 0.611. The Dice scores and the intersection over union (IoU) of each training epoch were calculated to quantitatively evaluate the model\u2019s performance (Fig.  3a ). During the training process, the IoU increased from 0.22 (epoch = 0) to 0.79 (epoch = 100), while the Dice scores exhibited a similar trend. These results demonstrated the capability of our fully automated algorithm to accurately segment PNF/MPNST on T2-weighted MRI images using a deep learning approach. Given the relatively small scale of the dataset, ResNet18 was adapted for further classification tasks. After training, the model automatically classified the 35 images (17 MPNSTs and 18 PNFs) in the test set to validate its performance. The prediction results of the ResNet18 model on the test set had an accuracy of 0.743, a recall of 0.882, and a specificity of 0.611. These data indicated that the AI model successfully differentiated MPNST from PNF (Fig.  3 ). The one-step whole-body tumor identification deep learning model exhibited excellent performance under the 1/3 computer power requirement compared to ResNet Both the U-Net and ResNet models represented a good balance between depth and performance. However, splitting the location and classification task into two separate processes did not mirror the clinical radiologist\u2019s diagnosis procedures, where recognition of highlighted areas on medical images is based not only on \u201cwhat they are\u201d, but also on \u201cwhere they are\u201d. Consequently, the one-step whole-body tumor identification model was developed. A total of 3019 images from dataset 1 with 10 labels were selected for training, while the remaining 131 images constituted the test set 1. Because the gland and lymph nodes exhibited similar MRI highlighted areas, the labels of the gland and lymph nodes were combined. The box loss values of the training set decreased from 6.27% to 2.79%, while they decreased from 4.30% to 2.47% in the validation process. Supplementary Fig.  1  and  Supplementary Data  show the results along training epochs. During the training process, the F1-score, PR (Precision-Recall), RC (Recall-Confidence), and PC (Precision-Confidence) curves further demonstrated the excellent discriminative performance of this model (Fig.  4c\u2013f ). Among all the labels, this model showcased the best ability to recognize eyes and teeth and encountered some challenges in identifying glands/lymph nodes. This aligned with our clinical experience that glands/lymph nodes, located throughout the body with various shapes, are more difficult to discern than eyes and teeth, which are positioned in fixed locations with highly consistent shapes that can be easily recognized. The model also exhibited better classification performance for MPNST compared to PNF (Fig.  5a ). Fig. 4 The training process of the one-step whole-body tumor identification deep learning model. a  The 10 labels of the training cohort and their respective instances. The center point of the image and the aspect ratio of the targets relative to the entire image were also exhibited.  b  A detailed diagram of the target distribution for image ( a ).  c \u2013 f  The F1-score, PR, PC, and RC curves for each individual label as well as the aggregate labels. The model exhibited higher performance in distinguishing MPNST compared to the classic two-step deep-learning model before. Furthermore, the model demonstrated diminished diagnostic precision for anatomically \u201ccomplex\u201d labels, such as the lymph/gland distributed throughout the body, in contrast to \u201csimple\u201d labels confined to specific regions like the eyes. Additionally, the model\u2019s performance was suboptimal for the bladder, potentially attributable to its underrepresentation in our cohort. Fig. 5 The results classified by the one-step whole-body tumor identification deep learning model. a  The confusion matrix of the training process.  b  The correct prediction percentages in the test set. An image might include several tumor lesions, and the percentage was calculated both along the number of images and tumor lesions at threshold values as 85%/90%.  c  Examples of the PNF images in the test set classified by the model.  d  The confusion matrix of the results of independent test set 2. The model showed an accuracy of 84.75% and a false negative rate of 15.25% in this independent test set.  e  Model performance comparison between the deep learning model trained on PNF/MPNST labels only and one that includes surrounding normal organ/tissue information. The model trained with information from surrounding normal organs/tissues enhanced diagnostic accuracy from 61.90% to 79.37% for MPNST and from 46.67% to 70% for PNF, affirming our hypothesis. Examples of the validation procedures on the test set 1 by the final model are shown in Fig.  5c , and the results were calculated and presented in Fig.  5b  and Supplementary Table  2 . If we set the threshold value of accuracy rates at 0.85, 36/42 MPNST (85.71%) and 66/89 PNF (74.16%) images were correctly diagnosed. By increasing the threshold value to 0.90, 33/42 MPNST (78.57%) and 54/89 PNF (60.67%) images were accurately identified. Notably, these achievements were accomplished with a model size nearly one-third of the previous ResNet model (14.7 MB of the one-step model compared to 44.8 MB of the ResNet model). The independent test set further substantiated the generalization ability of the one-step whole-body tumor identification deep learning model Given that the initial validation and test set 1 might incorporate images of the same individual from the training cohort, we employed an independent test set 2 (dataset 3) to validate the model\u2019s generalization capability. This test set comprised a total of 123 images (63 MPNSTs, 60 PNFs) from 20 patients (10 MPNSTs, 10 PNFs). For MPNST, the model correctly diagnosed 50/63 cases (79.37%), incorrectly diagnosed 9/63 as PNF (14.29%), and 4/63 lesions weren\u2019t recognized. For PNF, the model correctly diagnosed 28/60 cases (46.67%), incorrectly diagnosed 29/60 as MPNST (48.33%), and missed 3/60 (5.00%) lesions. The identification of false negative (FN) cases is another important quality metric in tumor screening. A FN rate of 14.29/15.25% (with/without missed lesions) indicated the potential of this model in MPNST screening (Fig.  5d ). For model interpretability, we represent the Gradient-weighted Class Activation Mapping (Grad-CAM) for the prediction process of MPNST images in the independent test set. The Grad-CAMs confirmed that the diagnosis of MPNST by our deep-learning model mainly relied on the characteristics of tumors themselves (Supplementary Fig.  2 ). In addition, when compared to another one-step model trained exclusively on PNF/MPNST labels (the training process was shown in Supplementary Fig.  3 ), incorporating information from surrounding normal organs/tissues enhanced diagnostic accuracy from 61.90% to 79.37% for MPNST and from 46.67% to 70% for PNF (Fig.  5e ). Moreover, we observed that the model exhibited reduced diagnostic precision in anatomically \u201ccomplex\u201d regions such as the head and face compared to \u201csimple\u201d areas like the legs. Only 1 in 10 MPNST leg lesions in the test set 1 had an accuracy rate below 0.85 (0.84), whereas this figure increased to 2 out of 8 for head and face MPNST lesions (0.72 and 0.80). These findings affirmed the hypothesis that surrounding information significantly improved the tumor identification ability of the AI model within a whole-body context. MRI-based AI differential diagnosis models are beneficial supplements to early clinical screening of MPNST beyond clinical symptoms In the lifelong follow-up of NF1 patients, presenting symptoms including spontaneous persistent pain, hard texture, rapid growth in a short period, and neurological deficits were regarded as preliminary indicators for MPNST . In the present study, we collected the clinical manifestations of patients from Shanghai Ninth People\u2019s Hospital. To align with each imaging dataset, information for individuals with multiple imaging records was replicated to match the MRI image set. In this way, the information from 159 patients with PNF and 19 patients with MPNST are presented in Supplementary Table  16 3 . At the time of telephone follow-up, 6 out of 19 MPNST patients (31.58%) had passed away, while none of the PNF individuals had died. Regarding clinical symptoms, all four models encompassed symptoms that demonstrated statistically significant differences between MPNST and PNF, consistent with prior studies. Nevertheless, our study included only a restricted number of MPNST patients, and the strategy of oversampling by duplicating information might further introduce bias into the data. An additional complication in symptom-based diagnosis was the overlap between PNF and MPNST, despite confirmed statistical differences. Furthermore, these phenomena are often found to be signs of advanced disease stages , constraining their use as early warning signs for malignant transformation of PNF. 13 , 17",
  "discussion": "Discussion Currently, there are two types of AI methods based on radiological imaging that are widely used . The first relies on image features defined by mathematical equations, such as tumor texture, which can be quantified algorithmically 1 , 18 . In NF1, two studies to date have employed machine learning to discern MPNST using MRI features 19 , 20 , highlighting the potential of the computer-extracted features in MPNST differential diagnosis. 21 , 22 Deep learning is the second method that has recently attracted considerable attention. To our knowledge, this study is the first attempt to implement an image-based deep learning approach combined with T2-weighted MRI to distinguish MPNST from PNF. Unlike most prior publications on other tumor types, AI model development for NF1 grapples with additional challenges, including a complex whole-body background and a limited dataset due to its rarity. Indeed, current AI studies on whole-body tumors are scarce, with a handful of studies demonstrating varied performance across different backgrounds . 23 , 24 In this multicenter study, we developed three deep learning models based on T2-weighted MRI images for PNF/MPNST segmentation and differential diagnosis under a whole-body background. These models were trained and evaluated using MRI images from seven hospitals. Notably, the YOLO-v5-based one-step model efficiently completed both diagnosis and segmentation tasks concurrently with high performance. The addition of contextual information further enhanced its accuracy. Additionally, an independent cohort was introduced to affirm the model\u2019s generalization capability, with an accuracy of 84.75% and a false negative rate of 15.25%, indicating its potential in MPNST early screening against a whole-body background. Meanwhile, for tumors that can manifest all over the body, such as metastatic cancers, this modeling approach and the concept of introducing normal surrounding organ/tissue labels might also be effective. For deep learning models developed in common disease, standalone software is normally developed for further clinical usage . Given that NF1 is a rare disease, standalone software is only suitable for research needs, making it challenging and cost-ineffective for broader clinical adoption. Therefore, our goal is to integrate the model as a plugin into existing software in the future, facilitating clinical use and promotion. This approach necessitates collaborations with imaging equipment manufacturers. Since these imaging software platforms are already established in hospitals, adding our model as a plugin allows for localized operation and naturally ensures patient privacy protection, fitting smoothly into hospital workflows without additional steps. A lightweight deep neural network in this study meets the demands and can serve as a reference for other rare diseases. 25 , 26 Several potential limitations should be considered in this study. First, the relatively small size of the rare disease cohort led to the possible inclusion of multiple image sets from the same individual. This might contribute to the machine learning part of the heterogeneities in the characteristics of the patients rather than tumors themselves, potentially inflating model accuracies. The model\u2019s high performance on an independent test set addresses this concern. Second, the dataset was divided into training and test sets at the image level. This approach differs from most previous studies developing deep-learning models for tumor identification within a single body region, which typically divided cohorts into training/validation sets at the patient level. Patient-level division could ensure that the proposed model can classify new patient data that it was not exposed to during training . In our study, however, imaging sets were not balanced across different body regions (as shown in Table  27 1 ), and dividing the dataset at the patient level randomly could potentially exclude PNF/MPNST in certain body regions from the training/test set. To avoid this while still maintaining research integrity, we split the dataset at the image level. Although relatively uncommon, image-level division has also been proved effective in some studies, offering another choice for deep-learning model establishment, especially for solving class imbalance . Moreover, we adopted an independent cohort for model test, and its high performance also helped to justify our partitioning strategy and support the generalizability of our one-step model. Third, the current models included in this study were \u201cblack box models\u201d, similar to most studies in this field, which caused distrust and raised concerns about clinical application. To address this, we focused on the training process outcomes and the predictive value of each included label rather than a simple \u201cto be or not to be\u201d consequence. In other words, we moved beyond binary predictions to assess trends in the model\u2019s outputs, analyzing how overfitting might affect each label\u2019s prediction. Meanwhile, we have conducted Grad-CAM (Gradient-weighted Class Activation Mapping) analysis on our independent test. Both of these could compensate for the deficiency of \u201cblack box\u201d model to some extent. Lastly, as this study focuses on a rare disease, we utilized MRI image sets from seven hospitals over a decade. As a result, inter-site variability emerged due to disparate devices and acquisition protocols, presenting a significant challenge in the context of a retrospective multi-center study. Nevertheless, the use of a diverse dataset also has its advantages, such as increased generalizability and applicability to real-world scenarios. 27 , 28 We suggest several potential directions for future research in this domain. Firstly, while this study includes validation using an external independent dataset, there remains a crucial need for prospective cohort validation to bolster the clinical relevance and dependability of our AI model. Throughout this prospective study, we will implement strategies to update and retrain our model iteratively. This process, often termed as continual learning or online learning, will enable the model to adapt seamlessly to new trends and patterns in the data, ensuring its sustained accuracy over time. One of the challenges of this prospective study is patient recruitment, especially for a rare disease, because the accuracy and stability of the deep-learning model rely on gold standards of surgical pathology diagnosis. Since MPNST is far less common than PNF, another multi-center collaboration over a long-term period is required. Another main challenge is how AI model validation can be conducted within the context of the model\u2019s intended clinical pathway, which has constantly advocated by members of the AI community (researchers, developers, guideline steering teams, etc.) . In real-world clinical practice, MPNST screening relies not only on T2-weighted MRI but also incorporates information from fMRI and clinical symptoms. This retrospective study demonstrated our deep-learning model as a potential tool for MPNST differential diagnosis. Future prospective studies should not only validate the model\u2019s dependability but also evaluate its performance in clinical scenarios by integrating other relevant clinical data. Secondly, upcoming studies should concentrate on explainable artificial intelligence (XAI). An XAI typically encounters a compromise between precision and interpretability, which warrants exploration in later studies. Thirdly, the current study only included T2-weighted images and relied mainly on morphological features. Ongoing efforts are directed towards integrating multimodality fusion images and incorporating additional information from DWI, enhancement patterns, and PET scans. Lastly, given the demonstrated efficacy of incorporating labels for normal adjacent organs/tissues in whole-body tumor identification, forthcoming research could investigate the development of models for additional tumor types found across the body. 29 In summary, our study successfully developed an MRI-based deep-learning model exhibiting robust performance in the automated segmentation and diagnosis of MPNST from benign PNF. Moreover, the successful development of this model indicates a potential approach to create AI models for other whole-body background primary/metastatic tumors.",
  "upgrade_date": "2026-02-21 02:19:28"
}