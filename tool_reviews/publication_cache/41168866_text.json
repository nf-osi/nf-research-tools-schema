{
  "pmid": "PMID:41168866",
  "title": "Artificial intelligence-based tools for precision diagnosis and treatment of neurofibromatosis type 1 associated peripheral and central glial tumors.",
  "abstract": "Modern Artificial Intelligence (AI) has demonstrated its effectiveness by achieving human-level performance in various complex tasks, including the biomedical field. Cancer research, adapting to a fast-changing world, is leveraging AI as a promising framework to better understand tumor development. Moreover, current AI methods can help predict more suitable and personalized treatment strategies for specific types of tumors. We explored AI methods applied to Neurofibromatosis Type 1, focusing on glial tumors. Additionally, we have reviewed all publicly available datasets to date. Discussion of future challenges is highly desirable since Neurofibromatosis Type 1 is one of the most common hereditary tumor syndromes and is associated with an increased rate of glial tumors as well as a reduced life expectancy due to malignancy. Not applicable.",
  "authors": "Fabio Hellmann; Inka Ristow; Lennart Well; Swanhild Lohse; Maxim Anokhin; Michaela Kuhlen; Elisabeth Andr\u00e9; Anja Harder",
  "journal": "Orphanet journal of rare diseases",
  "publicationDate": "2025-10-30",
  "doi": "10.1186/s13023-025-04093-5",
  "methods": "Review methodology This narrative review integrates clinical and technical dimensions of AI applications in NF1-associated glial tumors. The methodology was designed to address the heterogeneity of AI approaches across pathology, imaging, and molecular diagnostics while maintaining reproducibility. Search strategy A comprehensive search strategy was deployed across the databases: PubMed/MEDLINE and Google Scholar. The search period spanned from the database\u2019s inception through February 2025, with the final query executed on June 27, 2025. Keywords focused on the intersection of NF1, glial tumors, and AI techniques, e.g., specifically: \n \n Filters restricted results to human studies, English-language publications, and peer-reviewed articles/conference abstracts. Inclusion/exclusion criteria Inclusion criteria only considered primary research, clinical trials, and reviews addressing AI applications in NF1-associated peripheral/central glial tumors. Exclusion criteria removed animal studies, non-computational methods, and purely biological investigations. The study selection process included a two- to three-fold independent screening, as well as manual additions from SYNAPSE, the CTF data portal, and BioArchives, to collect technical reports and preprints. Quality assessment Technical studies underwent rigorous quality assessment using reproducibility metrics (code availability, hyperparameter documentation).",
  "cache_level": "full",
  "has_fulltext": true,
  "fetch_date": "2026-02-19 09:31:29",
  "introduction": "Background on artificial intelligence Artificial Intelligence is an essential tool in precision medicine, and its subfields - Machine Learning, Deep Learning, and Explainable Artificial Intelligence - are displayed in Fig.  2 . AI refers to the development of computer systems that can perform tasks typically requiring human intelligence, such as perception, reasoning, learning, and decision-making. Machine learning Machine Learning is a subset of AI that involves the development of algorithms that can learn from data and improve their performance over time without being explicitly programmed. ML algorithms can identify patterns in data and make predictions based on those patterns. However, before it is possible to identify those patterns accurately, it is crucial to engineer the features for training the ML algorithms. Feature engineering is the process - executed by a person or group of persons - of selecting, transforming, and extracting relevant features from raw data to improve the performance of ML models. It involves identifying and creating new informative, discriminative, and useful features for the learning task. Feature engineering aims to represent the data in a way that captures the underlying patterns and relationships, enabling the model to make accurate predictions or classifications. Feature engineering often requires domain expertise, creativity, and experimentation to achieve optimal results. After manually engineering the features, the ML algorithm learns in a supervised or unsupervised way. Supervised learning algorithms learn from labeled data by identifying the optimal output for each input. The algorithm uses this labeled data to learn a function that can map new inputs to the correct output. On the other hand, unsupervised learning algorithms do not rely on labeled data. Instead, they identify patterns and structures in the data to uncover hidden relationships or groupings. Generally, these types of models can be utilized for classification and segmentation tasks. Traditional segmentation techniques involve thresholding, region growing, and edge-based methods. These methods use simple intensity or gradient-based rules to partition images into distinct areas. ML-based methods can also be used for segmentation tasks, including decision trees, random forests, and support vector machines. These techniques leverage handcrafted features and trained models to classify pixels or voxels into different classes. Deep learning Deep Learning is a subset of ML that involves the development of neural networks, which are computational models inspired by the structure and function of the human brain. Deep Neural Networks consist of multiple layers, each processing the input from the previous layer. The first layer processes raw input data, such as an image or audio file or manually engineered features, and subsequent layers extract increasingly complex features from the input. The output of the final layer is the prediction made by the network. The feature engineering step is embedded into the DL algorithm itself. Accordingly, the DL algorithm extracts the relevant features from the raw data that seem fit to match the provided output labels. DL has proven to be particularly effective in tasks that require recognizing complex patterns, such as image and speech recognition. Like in ML, DL can also be used for segmentation by utilizing architectures like U-Net\u00a0 [ 14 ], Fully Convolutional Networks\u00a0 [ 15 ], and DeepLab\u00a0 [ 16 ]. These networks can learn complex spatial patterns and have demonstrated remarkable success across various medical imaging tasks. While many segmentation methods focus on 2D images [ 8 ], medical applications often require segmenting volumetric data in three dimensions. Techniques such as 3D convolutional networks and volumetric extensions of 2D architectures address the challenges of 3D segmentation [ 7 ,  9 ]. DL approaches offer increased accuracy, reduced human intervention, and the potential for real-time applications. Volumetric analysis and 3D imaging enable a comprehensive understanding of complex anatomical structures and pathologies. Techniques like 3D convolutional networks and region-based methods extend traditional 2D segmentation approaches to volumetric data, accounting for spatial context and continuity. Networks like 3D U-Net\u00a0 [ 17 ] and V-Net\u00a0 [ 18 ] leverage volumetric information to enable accurate segmentation, detection, and classification tasks. An emerging trend indicates that volumetric analysis involves integrating AI with virtual and augmented reality for immersive visualization. Explainable artificial intelligence Explainable Artificial Intelligence (XAI) is a field that seeks to make AI algorithms more transparent and understandable to humans by providing insights into how AI systems arrive at decisions. XAI is particularly important in fields such as healthcare, where it is essential to understand the reasoning behind an AI system\u2019s diagnosis or treatment recommendation [ 19 ]. One approach to XAI is to develop inherently interpretable models, such as decision trees or linear regression models [ 20 ]. These models are relatively simple and easy to interpret, allowing for a clear understanding of how they arrive at their decisions. Another approach to XAI is to develop post-hoc explanations for black-box models, such as DL networks [ 20 ]. These explanations seek to provide insights into why a particular decision was made by examining the weights and activations of the network. For example, in image classification, an XAI technique could highlight the image\u2019s most influential parts in the network\u2019s decision. An up-and-coming line of work in medical XAI involves the generation of counterfactual explanations, which aim to explain which minimal changes in the input would have led to a different diagnosis [ 21 ]. Such approaches can help both clinicians and patients better understand the model\u2019s behavior and its decision boundaries.",
  "results": "",
  "discussion": "Conclusion Among the discussed applications, radiomics-based classification is the most promising current application of Artificial Intelligence (AI) in the management of Neurofibromatosis Type 1 (NF1) tumors. Molecular genetic AI-based tumor classifiers are already in everyday use, but are not explicitly established for NF1-associated glial tumors. The same is true for AI-based classification from histological slides, which is rapidly growing in pathology; however, data on NF1-associated tumors are currently missing. There is also a gap in validated clinical AI applications for NF1, making this an important area of research for the future. To date, imaging heterogeneity, variable Magnetic Resonance Imaging (MRI) protocols, and small cohort sizes limit reproducibility and model generalizability. Moreover, ethical concerns such as data privacy and patient consent further complicate the issue of broad data-sharing. Model explainability and clinical trust are additional concerns, as clinicians must understand and validate AI-generated decisions to integrate them confidently into patient care. Future research should focus on developing multi-center, prospective trials incorporating AI tools into clinical workflows. Such studies should explore the longitudinal predictive value of imaging biomarkers, investigate delta radiomics (tracking feature changes over time), and integrate multi-omics data to support a truly personalized approach. Emphasis should also be placed on Explainable Artificial Intelligence (XAI) systems to ensure clinicians understand and trust the outputs. We strongly recommend fostering interdisciplinary collaboration between clinicians, data scientists, and AI developers to drive these advancements. Collaborative data platforms (e.g., SYNAPSE, CTF Data Portal) should be leveraged to harmonize datasets and annotation standards. Clinician input is vital for developing clinically relevant features and validating model outputs, while AI experts can ensure technical robustness and scalability. Establishing shared goals, communication protocols, and iterative development cycles will be essential for translational success. Additionally, developing interpretable AI systems is a challenging task. As more interpretable models may sacrifice accuracy, more complex models may be difficult to interpret. It will require clinicians and computer scientists to work closely to bring XAI to the clinical field and their domain perspective on explanations. Amann et al.\u00a0 [ 113 ] stated that the explanations must be suitable in their respective field of use. Several factors (e.g., time-critical, user knowledge) influence the need for explanations before actions are required.",
  "upgrade_date": "2026-02-20 07:33:17"
}